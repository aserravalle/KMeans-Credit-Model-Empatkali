 # -*- coding: utf-8 -*-
"""Prototype v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/114o-CAvMi4ZTOysH0e2atsph4tFr0yJp

"""
print('Setup')
#Manipulating dataframes
import pandas as pd
import numpy as np

# System Options
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Date and Time
import datetime
from datetime import timedelta

# Uploading user data from JSON files
import json
from pandas.io.json import json_normalize

from sklearn.cluster import KMeans

print('Reading Files')

user_json = []

for line in open('28Aug-UserAll.json', 'r'):
    user_json.append(json.loads(line))
user_raw = json_normalize(user_json[0])

shopper_json = []
for line in open('28Aug-Shopper.json', 'r'):
    shopper_json.append(json.loads(line))

# Convert to pandas
shopper = json_normalize(shopper_json[0])

email_raw = pd.read_excel('EmailLogs.xlsx')
TS_raw = pd.read_excel('trusrtingSocialData.xlsx')

def find_fate(df):
  for i in df.columns:
    print("'"+i+"':","'KEEP', ")

print('Cleaning User Data')

FATE = {
    		'_id': 'ID',
            'updatedAt':'DROP',
            'createdAt': 'DATE',
            'mobileNumber': 'phone',
            'card': 'KEEP',
            'danaVerifiedAccount': 'OHE',
            'emailVerified': 'OHE',
            'status': 'DROP', # Pending, approved, rejected etc.
            'remainingCredit': 'DROP',
			'activated': 'DROP',
			'credit': 'DROP',
            'registrationLoc.coordinates': 'KEEP',
            'registrationLoc.type': 'DROP',
			'verify.emergencyContact.passed': 'DROP',
            'verify.applicant.passed': 'DROP',
            'loc.coordinates': 'KEEP',
            'loc.type': 'DROP',
            'selfie': 'DROP', # Drop for now
            'npwp': 'DROP', # Mostly empty
			'defaultPayment': 'OHE',
            'emergencyContact.mobileNumber': 'DROP',
            'emergencyContact.name': 'DROP',
            'emergencyContact.type': 'OHE',
            'detail.gender': 'OHE',
            'detail.email': 'ID',
			'detail.birthdate': 'DATE',
			'detail.birthplace': 'DROP', #Drop for now
			'detail.name': 'DROP',
            'detail.penghasilan': 'OHE', # Income
            'detail.pendidikan': 'OHE',
			'detail.pekerjaan': 'OHE',
			'detail.industri': 'OHE',
            'ktp.number': 'DROP', # Mostly empty
			'ktp.image': 'DROP', # Drop for now
			'ktp.status': 'OHE', # What's this?
			'verify.applicant.notes': 'DROP',
            'verify.emergencyContact.notes': 'DROP',
			'detail.peghasilan': 'DROP',
}

# Read date and time
def dateparse(x):
  try:
    return pd.datetime.strptime(x,'%Y-%m-%dT%H:%M:%S.%fZ')
  except:
    try:
      return pd.datetime.strptime(x,'%Y-%m-%d %H:%M:%S')
    except:
      return np.nan

#Annoying missing values in CardType
def lowercase(x):
  try:
    return x.lower()
  except:
    return None

# Define data cleaning function
def Clean(df, FATE):
  DROP = []
  OHE = pd.DataFrame()
  coords_count = 0

  if list(df.columns) != list(FATE.keys()):
    raise Exception("Cleaning and fate columns aren't equal")

  for col in df.columns:
    if FATE[col] == 'DROP':
      DROP.append(col)

    elif FATE[col] == 'DATE':
      df[col] = df[col].apply(dateparse)
      temp_array = np.array([(i - datetime.datetime(1970, 1, 1)) / datetime.timedelta(days=1) for i in df[col]])
      df[col] = temp_array.astype('float')

    elif FATE[col] == 'phone':
      df[col] = df[col].apply(lambda x: str(x)[1:])
      df.rename(columns = {col:'Phone ID'}, inplace = True)
      FATE[col] = 'ID'

    elif (FATE[col] == 'OHE'):
      ohe = pd.get_dummies(df[col], prefix = col)
      DROP.append(col)
      OHE = pd.concat([OHE, ohe], axis=1)

  # Last
  df = pd.concat([df, OHE], axis=1)
  df.drop(DROP, axis = 1, inplace = True)
  return df

user = Clean(user_raw, FATE)

print('Location')

def parse_coords(series):
  Long = []
  Lat = []
  for row in series:
    if row in [[], np.nan]:
      Long.append(0)
      Lat.append(0)
      continue

    Long.append(float(row[0]))
    Lat.append(float(row[1]))
  return pd.DataFrame({'Long':Long,'Lat':Lat})

user[['Long','Lat']] = parse_coords(user['loc.coordinates'])
user[['Long_reg','Lat_reg']] = parse_coords(user['registrationLoc.coordinates'])

user.drop(['loc.coordinates','registrationLoc.coordinates'], inplace = True, axis = 1)

print('Unpack credit card details')

# pd.DataFrame(x) for x in user['card']
# This creates a list for each row in user.card
# Which is then turned to a dataframe

cards = pd.concat([pd.DataFrame(x) for x in user['card']], keys = user['_id']).reset_index(level=1, drop=True)
cards['type'] = cards['type'].apply(lambda x: x.lower())
cards = pd.concat([cards, pd.get_dummies(cards[['bank','type']]), ], axis = 1)
cards['type'] = cards['type'].apply(lambda x: x.lower())
cards = cards.groupby(['_id']).agg({'bank_VISA':'sum', 'bank_bni':'sum','type_credit':'sum','type_debit':'sum'})
cards.rename(columns={'bank_VISA':'VISA_cards', 'bank_bni':'BNI_cards','type_credit':'SumCredit','type_debit':'SumDebit'})

# Merge user and cards
user = pd.merge(user,cards, how = 'outer', on = '_id')
user.drop(['card','danaVerifiedAccount_False'], axis = 1, inplace = True)

# Fill missing bank
user.loc[:,['bank_VISA','bank_bni','type_credit','type_debit']] = user.loc[:,['bank_VISA','bank_bni','type_credit','type_debit']].fillna(0,axis = 1)

print('Cleaning Shopper')

FATE = {
    '_id': 'KEEP',
    'updatedAt': 'DROP',
    'createdAt': 'DATE',
    'total': 'KEEP',
    'transactionNumber': 'DROP',
	'status': 'DROP',
    'termins': 'KEEP', # Merge with shopper on...?
	'store._id': 'DROP', # what's this? Is there an index table?
	'store.merchant._id': 'DROP',
	'store.merchant.name': 'KEEP', # Do not OHE yet
	'store.name': 'DROP',
	'user._id': 'ID', # Merge with user on _id
    'user.mobileNumber': 'DROP',
    'user.selfie': 'DROP',
	'user.detail.email': 'DROP',
	'user.detail.name': 'DROP',
	'user.status': 'DROP'
    }

shopper = Clean(shopper, FATE)

merchants = pd.DataFrame(shopper['store.merchant.name'].value_counts())
merchants.to_csv('merchants.csv')

print('Unpack Termins')

# Unpack termins
termins = pd.concat([pd.DataFrame(x) for x in shopper['termins']], keys = shopper['_id']).reset_index(level = 1, drop = True)
termins.drop('_id', axis = 1, inplace = True)
termins.reset_index(inplace=True)

# Unpack due
due = termins.due.apply(pd.Series)
termins = pd.concat([termins,due], axis = 1)
termins =  termins.rename(columns = {'date': 'Due_Date', 'number':'Installment_Number','total':'Installment_Amount'})

# Unpack paid
paid = termins['paid'].apply(pd.Series)
termins = pd.concat([termins, paid], axis = 1)
termins.rename(columns = {'date': 'Payment_Date'}, inplace = True)

# Drop dictionary columns inside termins
termins.drop(['due','paid'], axis = 1, inplace = True)

# Clean
FATE = {
    '_id': 'KEEP', # Merge with shopper on this
    'Installment_Number': 'KEEP',
    'Installment_Amount': 'KEEP',
    'lateFee': 'KEEP',
    'Due_Date': 'DATE',
    'lastNotified': 'DROP',
    'notified': 'DROP',
    'lastChecked': 'DROP', # Drop - the whole thing is missing
    'lastFineCharged': 'DROP',
    'Payment_Date': 'DATE', # Fix this bad boy
    'method': 'OHE',
    'payment_id': 'DROP',
    'status_code': 'DROP',
    'status': 'KEEP', # Money received true/false
    'order_id': 'DROP',
    'paymentGateway': 'DROP'
    }
termins = Clean(termins, FATE)

# Target variable
termins['Lateness'] = termins.Due_Date - termins.Payment_Date

print('Summarise EmpatKali History')

# Merge all EmpatKali loan history
history = shopper.merge(termins, how='outer',on="_id")

# Get the days since the last loan
temp_array = np.array([(pd.to_datetime('today') - datetime.datetime(1970, 1, 1)) / datetime.timedelta(seconds=1)])
temp_array = temp_array/(60*60*24)
history['Last_Loan'] = temp_array - history['createdAt']

# Encode late fees paid
history['paidLateFee'] = np.where(history.lateFee > 0, 1, 0)

# OHE Merchant
ohe = pd.get_dummies(history['store.merchant.name'], prefix = 'Merchant')

# Drop columns you don't need anymore
drop = ['termins','_id','store.merchant.name']
history = pd.concat([history, ohe], axis=1).drop(drop, axis = 1)

# Rename
Names = {'user._id': '_id','createdAt':'Loan_Date','total':'Avg_Loan','status':'Paid'}
history.rename(columns=Names, inplace = True)

# Drop rows where lateness is missing
history.dropna(inplace=True)
history.reset_index(inplace=True, drop = True)

Agg =  {'_id': 'count',
        'Avg_Loan': 'mean',
        'lateFee': 'sum',
        'paidLateFee':'sum',
        'Installment_Amount': 'max',
        'method_MASTERCARD CREDIT': 'sum',
        'method_VISA CREDIT': 'sum',
        'method_credit_card': 'sum',
        'method_dana': 'sum',
        'method_vabni': 'sum',
        'Lateness': 'mean',
        'Last_Loan': 'min',
        'Merchant_Alvilabelonline': 'sum',
        'Merchant_Brodo': 'sum',
        'Merchant_DekadeTime': 'sum',
        'Merchant_Groot Pay Indonesia': 'sum',
        'Merchant_JW ETNIK': 'sum',
        'Merchant_KHAKIKAKIKU': 'sum',
        'Merchant_Karatvan': 'sum',
        'Merchant_Kasual': 'sum',
        'Merchant_OTIV': 'sum',
        'Merchant_Pale Buddy': 'sum',
        'Merchant_Phillip Works': 'sum',
        'Merchant_Sagara Boot Maker': 'sum',
        'Merchant_Saint Barkley Shoes': 'sum',
        'Merchant_Seymour': 'sum',
        'Merchant_Wellbourn': 'sum',
        'Merchant_bambooblonde': 'sum',
        'Merchant_dapoza': 'sum',
        'Merchant_footstep Footwear': 'sum',
        'Merchant_gudangimport': 'sum',
        'Merchant_guteninc': 'sum',
        'Merchant_kidzclusive': 'sum',
        'Merchant_tulisan': 'sum',
        'Merchant_vininc': 'sum'}

history2 = history.groupby(['_id']).agg(Agg).rename(columns = {'_id':'Count_Loans'})
history2.reset_index(inplace = True)

# Get a dataframe of all repayments merged with their transactions
user2 = user.merge(history, how = 'inner', on="_id")

print('Done: Cleaning Email')

tipe_ohe = pd.get_dummies(email_raw.tipe, prefix='email')
email = pd.concat([email_raw.email, tipe_ohe], axis =1)
email = email.groupby(['email']).sum()
email.reset_index(inplace = True)
email = email.rename(columns = {'email': 'detail.email'} )

user3 = user2.merge(email, how = 'left', on = 'detail.email')
user3.fillna(0,axis = 1, inplace=True)

print('Trusting Social')

# One hot encode credit score
TS = TS_raw.loc[:,['SCORE','MOBILE_NUMBER','SCORE_CATEGORY']]
dummies = pd.get_dummies(TS['SCORE_CATEGORY'], prefix = "TrustSoc_", prefix_sep = '')
TS = pd.concat([TS, dummies], axis = 1)
TS.drop('SCORE_CATEGORY', axis = 1, inplace = True)

# Prepare MOBILE_NUMBER for merge with user data
TS['Phone ID'] = TS['MOBILE_NUMBER'].apply(str)
TS.drop('MOBILE_NUMBER',axis =1, inplace = True)
user4 = user3.merge(TS, on = 'Phone ID', how = 'left')

# Drop the ID columns from training data
user4.drop(['detail.email', 'Phone ID'], axis = 1, inplace = True)

# Impute 0 or average for customers without trusting social data
user4.loc[:,['TrustSoc_A','TrustSoc_B','TrustSoc_C','TrustSoc_D']] = user4.loc[:,['TrustSoc_A','TrustSoc_B','TrustSoc_C','TrustSoc_D']].fillna(0)
user4.loc[:,['SCORE']] = user4.loc[:,['SCORE']].fillna(user4.SCORE.mean())

today = datetime.date.today().strftime('%Y%m%d')
user4.to_csv('Train_'+today+'.csv', index=False)

print('K Means Clustering')

# Fit KMeans
X_train = user4.drop('_id', axis = 1)
KMns_Args = {'n_clusters': 3, 'random_state': 1}
kmeans = KMeans(**KMns_Args)
kmeans.fit(X_train)

for clust in range(3):
  count = np.count_nonzero(kmeans.labels_ == clust)
  print('Class',str(clust),'- Count',str(count))

# Group data by cluster
user5 = user4.assign(Cluster = kmeans.labels_)
user5 = user5.groupby(['Cluster']).mean()
user5.to_csv('Clustered_'+today+'.csv', index=False)
